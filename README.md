# CPS 491 - Capstone II

Source: <https://github.com/cps491sp21-team13/cps491sp21-main-repo.git>

University of Dayton

Department of Computer Science

CPS 491 - Capstone II, Semester Year

Instructor: Dr. Phu Phung


## Capstone II Project 


# Compressible Learning Agents for Autonomous Cyber-Physical Systems

# Team members

1.  Zachary Rowland, rowlandz1@udayton.edu


# Company Mentor

Matthew Clark, Principal Scientist

Galois

444 E 2nd Street

Dayton, OH 45402


# Project Management Information

Management board (private access): <https://trello.com/b/kVF2rdUV/cps-491-team-13>

Source code repository (private access): <https://github.com/cps491sp21-team13/cps491sp21-main-repo>

Project homepage (public): <https://cps491sp21-team13.github.io/>

## Revision History

| Date       |   Version     |  Description |
|------------|:-------------:|-------------:|
| 16/02/2021 |  0.1          | Details of Taylor and convolutional approximations |


# Overview

The goal of this project is to explore and document the correlation between the configuration of deep neural
networks and the analytical mathematical functions that they model. A long term goal, possibly not explored
in this project specifically, is to develop methods for _decomposing_ an arbitrary deep neural network into
smaller networks with intuitive behavior, and _simplifying_ the network into the minimum network needed to
model the desired mathematical function.

# Project Context and Scope

This project is a contribution to research into the development of machine learning models that learn intuitively.

## High-level Requirements

By the end of the semester, we hope to have reasonably comprehensive documentation of the different ways simple neural networks can approximate simple functions. The process of compiling this documentation should involve careful inspection of trained neural networks guided by mathematics.

# Implementation

Most work on this project is being done in a Jupyter notebook using markdown for mathematics
and Tensorflow for implementing neural networks.

# Software Process Management

## Scrum process

### Sprint 0

Duration: 20/01/2021-27/01/2021

#### Completed Tasks: 

1. Studied fuzzy inference systems from material provided from Galois
2. Studied automatic differentiation.

#### Sprint Retrospection:

| Good     |   Could have been better    |  How to improve?  |
|----------|:---------------------------:|------------------:|
| learned a lot of preliminary information | documentation of progress | take more notes on what I read about/discover  |

### Sprint 1

Duration: 27/01/2021-03/02/2021

#### Completed Tasks:

1. Learned tensor manipulation and learning models in Tensorflow
2. Experimented with small neural networks in a Jupyter notebook

#### Sprint Retrospection:

| Good     |   Could have been better    |  How to improve?  |
|----------|:---------------------------:|------------------:|
| good pace aquiring new knowlege and documenting what I have learned | could have started experimenting with small networks sooner | reach out to ask questions sooner |

### Sprint 2

Duration: 03/02/2021-03/09/2021

#### Completed Tasks:

1. Learned about convolution from Linear Systems.
2. Detailed a technique for constructing neural networks that approximate arbitrary
   functions inspired by convolution.
3. Detailed a technique for approximating arbitrary polynomials with neural networks
   based on Taylor-series expansion.

#### Sprint Retrospection:

| Good     |   Could have been better    |  How to improve?  |
|----------|:---------------------------:|------------------:|
| documentation was thorough and clear | direction of work could have been more focused toward project goal | more formal communication about the precise goal of the project |

# Acknowledgments 

I would like to thank Mr. Matthew Clark, Principal Scientist at Galois, for mentoring this project.
